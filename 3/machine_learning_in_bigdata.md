## Fundamentos del aprendizaje automático (machine learning)

### 1.1 Aprendizaje automático y big data

#### Introducción al aprendizaje automático en relación al big data


Aprenderéis algunas de las herramientas y técnicas para el análisis de grandes volúmenes de datos mediante el uso de técnicas de aprendizaje automático, machine learning. Desarrollás, sobre la base de lo aprendido anteriormente, los datos preparados, y mirar las relaciones más profundas entre elementos. Y también a construir, ejecutar e interpretar algunos de los principales modelos de aprendizaje automático.

En primer lugar, vamos a recapitular dónde estamos en este momento. Recordaremos una buena estrategia para hacer frente a grandes volúmenes de datos en el framework **MapReduce** y el sistema de archivos **HDFS**.
Recordaremos también que el ecosistema de Hadoop proporciona herramientas para ayudar a procesar grandes volúmenes de datos y hacer el framework más flexible.

Estas herramientas nos ayudan a realizar el procesamiento de consultas, reunir resúmenes, explorar los datos.
Otro tipo de análisis que vamos a cubrir en este curso es el de **modelado**.

>A un alto nivel, podemos pensar en el modelado como todo aquello que trata de encontrar `relaciones` entre los datos. Esto podría implicar categorías de datos, o entre las variables en los datos, o entre las variables en alguna etiqueta de destino o de clase externa.

Y las `relaciones` o bien pueden ser provistas por un experto en el dominio, o que se puedan aprender, y esencialmente eso es el aprendizaje automático. Aprenderás algunas de las claves, algoritmos principales, y en qué contexto se deberán usar. Se va a cubrir el aprendizaje automático utilizando una herramienta basada en GUI llamada Nine, que es posible ejecutar con datos resumidos. Y con la plataforma Sparks, que trabaja directamente con datos distribuidos.

#### Aprendizaje automático con big data: análisis avanzado de datos

Hemos hablado mucho de tiempo real, de plataformas de alto rendimiento. Hemos hablado de Hadoop y Spark. Hemos aprendido hasta ahora cómo incorporar y manejar los datos en estas plataformas. Cómo hacer analíticas básicas. Algunos resúmenes básicos. Ahora vamos a introducirnos y mirar en las capas adicionales para el análisis de estos grandes volúmenes de datos. Hemos hablado de que los datos están creciendo muy rápido, que van desde terabytes y petabytes hasta zettabytes y exabytes. Hemos hablado acerca de la velocidad con la que todo este desarrollo está avanzando, y de las plataformas y las técnicas que pueden ayudarnos a domesticar esta noción de big data. Anteriormente, hemos pasado tiempo aprendiendo sobre cómo organizar y realizar consultas, y ambas son piezas importantes dentro del procesamiento en big data. Ahora vamos a añadir otra capa encima de eso, denominada analítica avanzada (**advanced analytics**).

Vamos a empezar a hablar de la minería en el entorno big data, así como de la clasificación, de la regresión, de las reglas en minería de datos, clustering, todas diferentes técnicas que podemos aplicar a estos datos para tratar de obtener alguna información adicional.

Anteriormente, se ha hecho mención de un par de cosas diferentes sobre los grandes desafíos de datos. ¿Cuáles son algunos de los grandes desafíos de datos? Bueno, en primer lugar, se trata de encontrar el talento. Y es de esperar, dado que nosotros mismos nos estamos enfrentando ahora a este reto al tomar este curso de especialización en big data. En segundo lugar, hemos estado recogiendo datos de diferentes fuentes. Nos han estado llegando estos diferentes conjuntos de datos de diferentes fuentes, como Twitter y Yelp y otros a lo largo del recorrido de esta especialización. Vamos a seguir haciendo eso.

Y el tercero consiste en entender las herramientas y plataformas. Aquí, vamos a aprender acerca de las herramientas específicas y plataformas específicas para la búsqueda de una visión de los datos mediante el uso de minería de datos, sistemas predictivos y todo aquello que proporcionan las técnicas de aprendizaje automático. Así, transformar los datos en conocimiento para tomar mejores decisiones es nuestro objetivo. Vamos a empezar a partir de datos, vamos a aplicar algún tipo de análisis, vamos a ver Insight, sin olvidar que nuestro principal objetivo es tomar decisiones basadas en estos datos y que nos conduzcan a tomar acciones.

Para ello, hemos hablado un poco sobre esto anteriormente y hemos mencionado enfoques de análisis tradicionales. Un enfoque de análisis tradicional es el que típicamente realizan las empresas que tienen una gran cantidad de datos en un número de formas diferentes y que tratamos de dar sentido fuera de él para ver cómo pueden ayudar a su negocio. A menudo, se requiere de muchos tipos diferentes de análisis en función del problema que se está tratando de resolver. Algunos análisis utilizarán un almacén tradicional. Tal vez, en otro tipo de análisis, se requerirán muchos enfoques diferentes para traer diferentes conjuntos de datos. Pero, típicamente, solemos tener una gran cantidad de datos, tomar algunos subconjuntos de los datos y tratar de ver si podemos revelar algunos patrones. De esta forma, podemos tomar esos patrones y llevarlos de vuelta al negocio permitiendo algunas nuevas funcionalidades o proporcionar alguna información adicional.

Ahora bien, si empezamos a hablar de los análisis de big data y el enfoque en big datas, las cosas están cambiando, tenemos que realizar un poco de cambio en el paradigma de trabajo. La capacidad para gestionar y analizar petabytes de datos permite a las empresas ahora estudiar los grupos de información que puedan tener un impacto en el negocio. Y ahora estamos requiriendo estos nuevos motores de análisis que pueden gestionar esta información que se encuentra altamente distribuida. Estos motores pueden proporcionar resultados con el fin de optimizar y resolver muchos diferentes retos empresariales. Así que ahora el análisis puede llegar a ser muy complejo con big data.

Por ejemplo, muchas organizaciones están utilizando modelos predictivos que emparejan tantos datos estructurados y no estructurados en conjunto para predecir el fraude. Análisis de medios sociales, análisis de texto, todos estos nuevos tipos de analíticas están siendo utilizados debido a la presencia del big data, y fundamentalmente debido a todas estas nuevas tecnologías y plataformas que nos permiten acceder al procesamiento distribuido sobre este gran conjunto de datos.

![](/img/1.png)

Así que hemos visto este diagrama antes. Hemos hablado de estos diferentes niveles de datos y cómo llevan los datos a las decisiones y a la acción. En lo que nos vamos a centrar ahora es en el tercer nivel, en el predictivo. ¿Qué va a pasar ahora? ¿Cómo podemos tomar las predicciones y que se conviertan en decisiones y luego en posteriores acciones? Esto es en lo que vamos a centrar nuestro estudio en su mayoría. Anteriormente mencionamos la forma en que todo encaja en este nivel de análisis de madurez. Ya hemos hablado de analizar los datos de que disponemos, de ser capaces de hacer las preguntas, por qué las cosas suceden, lo que sucedió. Y ahora estamos hablando de modelos preictivos que nos ayudarán a responder a las preguntas de lo que podría suceder. Y tal vez por qué y cómo está sucediendo. Y entonces podemos entrar en este aún más alto nivel de optimización. Lo que llamamos análisis predictivo prescriptivo u optimizado.

Entonces, ¿cómo la máquina de aprendizaje encaja en todo el gran esquema de big data? Hasta ahora, hemos aprendido mucho sobre grandes volúmenes de datos. Sabemos que hay una gran cantidad de datos estructurados, una gran cantidad de datos no estructurados, podemos saber que hay una gran cantidad de datos de los medios sociales y nos vamos a poder traer todos estos datos juntos. Vamos a aplicar una serie de diferentes algoritmos de aprendizaje automático con el fin de encontrar Insight. Hay muchos tipos diferentes de aplicaciones de datos grandes. Tradicionalmente, el negocio espera que se utilizarán los datos para responder a ciertos tipos de preguntas acerca de qué hacer, tal vez cuándo hacerlo, cómo son intrínsecamente los datos. La integración a menudo se hace de muchas maneras diferentes y viene a través de muchos campos diferentes, dependiendo realmente de si se trata de una aplicación empresarial de propósito general o de aplicaciones de negocio muy específicas. Con la llegada de big data esto está cambiando. Ahora estamos viendo el desarrollo de aplicaciones que están diseñadas específicamente para aprovecharse de grandes volúmenes de datos. Para aprovechar estas características específicas de grandes volúmenes de datos. Y vemos continuamente más y más de estos en muchas áreas. Más específicamente, salud, manufactura, comercialización, están desarrollándose y están enfocados en utilizar grandes cantidades de datos específicos. Lo que todas estas aplicaciones tienen en común es que, si recordáis que hablamos sobre las tres V, que están todos los grandes volúmenes de datos, velocidad y variedad de datos. Además, la posibilidad de transformar los datos y el comportamiento que aprenden sobre esos datos es lo que se transforma en la ventaja para el mercado.

Por lo tanto, la única cosa que la mayoría de estas aplicaciones tendrá en su fundamentación es estar construido bajo el paradigma de algoritmos de aprendizaje automático. Así que vamos a ver estos algoritmos de aprendizaje automático y cómo podemos utilizarlos.

#### Una mirada más cercana al aprendizaje automático

La gente, históricamente, ha empezado a utilizar y a adoptar nuevas técnicas de la inteligencia artificial y del aprendizaje automático desde los sistemas de informática y computación. Y, con el tiempo, algunas de estas técnicas se han elegido y utilizado por un número de diversas industrias, comenzando con la predicción del mercado de valores, predicciones sobre fraude y cosas por el estilo. En ese momento se adoptó el nombre de minería de datos. Hablaremos de ello dentro de poco, aunque solo existe la intención de que os familiaricéis con esta terminología. Con el paso del tiempo, la minería de datos comenzó a evolucionar en análisis predictivo, que estaba en su mayor parte basado en estos algoritmos de aprendizaje automático junto con quizás algunas de las herramientas de business intelligence (BI). Entonces esos términos comenzaron a evolucionar en algo que se llama advanced analytics. Y actualmente oímos mucho hablar sobre la ciencia de datos. Por lo tanto, cuando oímos hablar de la máquina el aprendizaje, la minería de datos, el análisis predictivo, a veces la gente utiliza estos términos indistintamente, y probablemente no presenta ningún riesgo seguro para el propósito de nuestro aprendizaje. En realidad, nos estamos preparando para estas analíticas más avanzadas y hacia el sentido de datos en el que estamos trayendo toda esta tecnología y los problemas de negocios a la vez.
Así que hay muchas definiciones de la minería de datos, del aprendizaje automático.

>Data mining (minería de datos) es el proceso de extracción de información significativa de grandes bases de datos, información que revela inteligencia del negocio, a través de factores ocultos, tendencias y correlaciones para permitir al usuario realizar predicciones que resuelven problemas del negocio proporcionando una ventaja competitiva. Las herramientas de data mining predicen las nuevas perspectivas y pronostican la situación futura de la empresa, esto ayuda a los mismos a tomar decisiones de negocios proactivamente.

![](/img/2.png)

Probablemente hay otras cien definiciones diferentes, y es probable que haya algunas definiciones mejores. Pero creo que realmente muestra así que de lo que estamos hablando aquí es del impulso de descubrimiento de información en los datos, y el modelado de patrones ocultos en los grandes volúmenes o datos. Nos está hablando sobre implícito, previamente desconocido, inesperado, potencialmente muy útil, o estamos hablando de minería de datos en diferentes formas, y también, como podremos ver posteriormente, que pueden ser en términos de reglas o irregularidades, tal vez patrones, tal vez limitaciones en datos fuera de rango. Pero por lo general son todos ellos procedentes de cualquiera de las bases de datos, o grandes conjuntos de datos filtrados, o HDFS, o muchas fuentes diferentes.

Así que cuando hablamos de minería de datos, hemos querido hablar acerca de esta metodología de realización de arriba abajo. Digamos que tenemos una gran cantidad de datos y nos gustaría utilizar algunas herramientas analíticas.

Si empezamos usando herramientas de SQL, puede que la ejecución de algunas consultas SQL, estamos haciendo algunos informes, decimos que estamos arañando la superficie de la información de nuestros datos. Debido a que estamos en busca de las cosas que sabemos que están ahí, y queremos encontrar específicamente qué tipo de datos y cómo se aplican. A continuación, vamos a entrar en un análisis algo más profundo. Quizá utilicemos algunas herramientas estadísticas y BI. Tal vez estemos haciendo algunos resúmenes y análisis. Y entonces, vamos a centrarnos, en este estudio específico, en esta información oculta. Estamos hablando de estos métodos de minería de datos ascendentes para el descubrimiento de conocimiento. Así que vamos a necesitar cavar profundamente en nuestros datos tratando de encontrar estos patrones que tal vez no sabíamos que existían antes.

![](/img/3.png)

A menudo, vais a ver este tipo de gráficos en el que se habla sobre papel pasivo y activo de software, así como la presentación, la exploración y el descubrimiento de los datos. Y lo que donde realmente queremos estar es en esa esquina superior derecha. Realmente queremos tratar de llegar a ese análisis predictivo o minería de datos, junto con la herramienta de BI que nos permitirá subir con más descubrimiento avanzado, más el conocimiento del negocio, y un enfoque más proactivo para nuestros datos. Entonces, ¿qué es la minería de datos? 

>Realmente es una combinación de la IA o herramientas de inteligencia artificial y el análisis estadístico que están utilizándose a la vez para descubrir la información oculta en nuestros datos.

Entonces, ¿qué tipo de cosas se pueden descubrir a partir de datos? Existen diferentes tipos de relaciones que podemos descubrir en nuestros datos. Una de ellas son las asociaciones, y tal vez hayáis oído hablar sobre la historia de la pizza y la cerveza; algunas personas dicen que no es una historia real. La historia comienza cuando alguien está en la tienda de comestibles, mientras nosotros estamos buscando patrones en los datos, se dan cuenta de que cuando los hombres fueron de compras los jueves y los sábados, entonces coincide que compraron cervezas y pañales. Y no existía previamente un enlace entre la cerveza y los pañales, y ahora se pueden realizar todo tipo de análisis de la información con esto y vamos a hablar sobre este asunto un poco más cuando entremos en la asociación de en la asociación de.

* Entonces podría estar buscando secuencias. Tal vez estamos tratando de atar eventos juntos. Matrimonios, compras de muebles, tal vez hubo algún componente de tiempo allí también. 
* O que podría estar buscando alguna clasificación, reconocimiento de patrones, tales como quizás los atributos de los empleados que tienen más probabilidades de dejar de fumar, o atributos de los clientes que tienen más probabilidades de atraer a nuestra empresa en particular. 
* Entonces podríamos estar mirando la previsión y la predicción de los hábitos de compra de los clientes sobre la base de sus patrones del pasado. 

Así que hay una gran cantidad de diferentes tipos de visión oculta en nuestros datos que podríamos estar buscando y descubriendo. Anomalías, valores atípicos, fraudes, muchos tipos diferentes de cosas que podemos conseguir y analizar. A menudo, nos vamos a centrar en el agrupamiento. Podríamos encontrar grupos o segmentos de nuestros clientes, o clústeres como nos referimos con más propiedad a estos conjuntos, que tienen características similares. Y nos gustaría saber cuáles de esas características suceden y cómo distinguirlas de otros subgrupos de nuestros clientes.

También es importante saber esto: lo que la minería de datos no es. 
> La minería de datos no es el almacenamiento de datos o el procesamiento de consultas. Aunque utilizamos muchas veces estas dos tecnologías para extraer los datos. Podríamos construir la minería de datos en un sistema experto o una aplicación o software como servicio.

El procesamiento analítico en línea OLAP o las diversas herramientas de BI son dos técnicas que, a menudo, se utilizan a la vez o trabajando junto con los datos en el proceso de minería de datos. Y eso es genial, pero por sí mismos no van a realizar minería de datos para vosotros. Lo mismo sucede con la visualización de datos o los flujos de trabajo genéricos. Podemos traer toda la visualización y los flujos de trabajo en conjunto para apoyar tal vez algunos de estos algoritmos de aprendizaje automático.

Es un campo multidisciplinario. Se traen tecnologías de bases de datos, que van a traer herramientas estadísticas y, en teoría, algunos de los algoritmos de aprendizaje automático, algunos de los algoritmos de inteligencia artificial. Definitivamente, utilizan una gran cantidad de visualización, mirando los datos desde diferentes perspectivas.

#### Taxonomías en el aprendizaje automático

Vamos a hablar de dos categorías principales de estos modelos de aprendizaje automático. Una se llama **métodos predictivos**. Y estos métodos utilizan algunas de las variables para predecir algún tipo de conocimiento aún desconocido o un valor futuro de otras variables. Por lo que son muy predictivos por propia naturaleza, estamos tratando de predecir un valor que nos interesa. Y en realidad no es demasiado importante que lleguemos a esa respuesta, solo queremos la mejor predicción posible. Sin embargo, algunos de los modelos de predicción podrían ser descriptivos también. Lo que queremos decir con esto es que los **métodos descriptivos** pueden encontrar patrones humanos interpretables que pueden describir correctamente el comportamiento de los datos. Así que podemos mirar este modelo y realmente podemos entender en qué forma se está desarrollando esta caracterización de los datos. O cómo se están adquiriendo algunas de estas propiedades generales de los datos. Cuando estamos hablando de la minería predictiva, estamos realmente interesados en la forma de inferencia de los datos actuales y cómo conseguir que se realicen esas predicciones. Algunos de los algoritmos sobre los que vamos a hablar podrían tener ambas propiedades. Y vamos a hablar de ello, ya que cubrimos cada uno de nuestros algoritmos.

Otro tema interesante es machine learning (ML) supervisado frente no supervisado. Si hablamos de **aprendizaje supervisado**, se proporciona normalmente un conjunto de datos de entrenamiento con ejemplos etiquetados. Se puede pensar en él como tener un profesor o supervisor, un experto que ha de corregir nuestras respuestas. Y nosotros utilizaremos esos datos para capacitar a nuestros modelos y encontrar patrones. Por lo general, nuestros vectores de entrada se proporcionan junto con su correspondiente variable de destino o clase. Y nuestro objetivo es predecir la clase a evaluar. Ahora bien, si estamos hablando de aprendizaje **no supervisado**, los datos que se proporcionan por lo general lo hacen sin ningún conocimiento a priori de cualquier información sobre los datos, excepto los datos en sí. Así que no tenemos información acerca de los patrones ocultos que pueden estar contenidos por debajo de estos datos. No hay ningún valor de la clase o el valor de salida proporcionada por cada uno de nuestros vectores o instancias. Y decimos que los datos no están etiquetados, a veces se oye la palabra sin etiqueta también. Y por lo general, estamos usando esto en las aplicaciones de los cuales los datos de entrenamiento comprenden ejemplos del vector de entrada sin ningún tipo de variable de destino correspondiente. Y el objetivo es encontrar los patrones de forma natural concurrentes, tales como agrupaciones o clústeres, o segmentación. ¿Cómo funciona la máquina de aprendizaje? Acabamos de mencionar un poco que queremos explorar los datos que tenemos, queremos encontrar algunos patrones. A menos que encontremos esos modelos y patrones, a continuación, vamos a tratar de realizar predicciones sobre los nuevos datos que no se ven.

Entonces, ¿qué tipo de conocimiento se puede encontrar desde la minería de datos? ¿Qué pueden los algoritmos de minería de datos descubrir por nosotros? Muchas veces vamos a oír hablar sobre estos modelos predictivos que acabamos de mencionar. Pero también hay varias categorías diferentes de modelos de predicción. Tenemos clasificación, regresión y predicción. 

* En la **clasificación** normalmente el sistema aprende cómo particionar los datos. Y por lo general, ofrecemos al sistema ejemplos u objetos de diferentes grupos. Y luego dejamos que el algoritmo decida sobre el perfil de cada grupo en función de los campos y atributos que son exclusivos de ese grupo específico. Y a continuación, el algoritmo puede aprender y decidir qué datos pertenecen a los nuevos grupos, y adicionalmente realimentamos el sistema.

* Podríamos querer estar interesados en los **modelos descriptivos**. Y los modelos descriptivos son por lo general los modelos que pertenecen a la categoría de cualquier análisis de conglomerados o segmentación. Y estos modelos son modelos sobre los que estamos tratando de aprender agrupaciones específicas, las características de la agrupación de los datos que tenemos.

* También podemos estar interesados en el **descubrimiento de patrones y reglas**. Podríamos querer que nuestro sistema explorara los datos y tratara de encontrar si hay alguna relación entre los diferentes atributos. A continuación, averiguaría si la presencia de un elemento implica la presencia de otro elemento. Por ejemplo, tal vez el 50 % de todas las personas que compran leche también compran cereales. Y entonces, tal vez podría haber otras reglas de dependencia que a su vez dependieran de la secuencia de las reglas que se producen. Por lo que el sistema puede buscar los datos y tratar de identificar patrones repetidos y la presencia de un conjunto de patrones siga este otro conjunto de patrones. 
* O podría ser un aspecto de **series de tiempo** con nuestros datos. Y entonces estamos hablando de series de tiempo en que estamos tratando de identificar cualquiera de las apariciones regulares o secuencias en el tiempo.
* Además, muchas veces podemos estar interesados en la **detección de la desviación**. El sistema puede determinar si se ha producido un considerable cambio en un atributo del valor anterior o esperado. Como os podéis imaginar, esto es a menudo útil para la adición de otras aplicaciones de este tipo. 

Así que la gente a menudo se pregunta: ¿dónde está la minería de datos o la máquina de aprendizaje aplicados a este campo? Y se aplica más o menos en todas partes, en cualquier lugar de la ciencia, incluyendo la química y el análisis de bioquímica. Los sensores remotos y satélites, los análisis de imágenes médicas, todo el camino hasta el análisis basado en la secuencia. Estructuras de proteínas, clasificación de la familia de proteínas, genes de microarrays. Entonces, podemos hablar sobre las farmacéuticas, los seguros, el desarrollo de fármacos, la terapia médica, las conductas fraudulentas, los diagnósticos. Todos los tipos de aplicación hasta el fondo de la industria financiera y los bancos y las empresas, y el comercio electrónico, análisis de mercado y de gestión. Para el análisis de riesgos y la gestión y la previsión de retención de clientes, el control de calidad, el análisis de la competencia, la gestión de riesgos, la previsión de ventas, el mercado de valores, lo que sea. ML más o menos se ha aplicado a cualquiera de estos campos, e incluso a los deportes y entretenimiento.

Algunas de las aplicaciones más interesantes se encuentran en los hospitales. Hemos visto algunas aplicaciones en las que se utilizan los árboles de decisión para predecir si las entidades de crédito deben darle un préstamo. Call centers, líneas aéreas, hemos visto que el aumento de los ingresos y la satisfacción del cliente, al tratar de predecir qué pasajeros son más propensos a perder sus vuelos.

### 1.2 Minería de datos y su relación con el aprendizaje automático

#### Tareas en la minería de datos


Vamos a ver un par de tareas críticas en el proceso de encontrar un conjunto de módulos o funciones que describen y distinguen las clases o conceptos de datos para poder utilizar el modelo para predecir la clase. O el conjunto de objetos cuya clase no se conoce o no se ha etiquetado aún.

Podemos utilizar los modelos adecuados cuya base es **la analítica**. Y en su lugar, tomar un conjunto de datos de entrenamiento, que vamos a tomar de algún conjunto de objetos en el que ya habíamos etiquetado sus clases. Vamos a aplicarlo, vamos a correr los datos y el conjunto de entrenamiento etiquetado a través del algoritmo de aprendizaje automático para poder aprender a distinguir y predecir entre ellos, y entonces podremos tomar estos valores predichos en concreto o sobre algunas características específicas. Así podemos pensar en la clasificación y predicción como modelos o funciones de búsqueda que pueden distinguir estas diferentes clases. Y el gran ejemplo de esto es la clasificación de los países según el clima o la clasificación de coches basados en el kilometraje. Lo que es realmente bueno de estos modelos es que se pueden representar corrientemente como reglas si-entonces, y de esta forma resultan descriptivos por lo que el ser humano puede mirar el modelo, entenderlo, y ser capaz de interpretarlo.

Otra tarea interesante es **la asociación**. Nosotros, con frecuencia, hablamos de correlación y causalidad. Y correlación no implica necesariamente causalidad. Muchas de estas reglas de ML están buscando correlaciones en los datos. Y entonces tenemos que mirar en ellos y tratar de explicar por qué creemos que esas correlaciones están sucediendo. Por lo tanto, muchas veces el análisis de asociación es el descubrimiento de reglas de asociación que están mostrando las condiciones de valores de atributos que se produjeron con frecuencia juntos en un conjunto de datos dado. Por lo que se puede, por ejemplo, imaginar que tal vez si estáis en un determinado grupo de edad y tenéis cierto nivel de ingresos, es más probable que compréis un tipo particular de televisor o un tipo particular de coche, de forma que se puede pensar en la generación de este tipo de normas que están llegando a lo largo de los datos y podemos descubrir estos patrones desconocidos a partir de los datos.

A diferencia de la clasificación y predicción, en el clustering se analiza la clase que tiene que etiquetarse. Esto se llama aprendizaje supervisado (supervised learning). Objetos de datos en la agrupación han llegado sin ningún tipo de etiquetas, tenemos solo el conjunto de objetos y nos gustaría agrupar los datos para formar nuevas clases. Así pues, estas agrupaciones también conocidas como algoritmos de agrupamiento pueden darse en circunstancias importantes en las que no conocemos lo que, por ejemplo, una rana se parece. Pero nos gustaría identificar a los grupos pequeños tal vez, sospechosos grupos de personas. 

Por otra parte, podemos estar interesados en la segmentación general de nuestros clientes y queremos ver qué tipo de datos demográficos u otros aspectos interesantes de nuestros clientes van de la mano o en grupo juntos. Muchas veces, no estamos buscando patrones generales sino que estamos buscando esos otros pocos datos fuera del análisis ordinario, lo que llamamos análisis de valores atípicos. Aquí es donde el objeto de datos no cumple con el comportamiento general de los datos. Que es todo lo contrario de todas las reglas de agrupamiento y clasificación y las reglas de asociación que estábamos hablando. Entonces fueron acertadas en la búsqueda de patrones generales de los que se alimentaban nuestros datos. Cuando estamos hablando de análisis de valores atípicos, estamos muy a menudo trabajando en ese pequeño porcentaje que no cumple con el comportamiento general, y que si se equipara con su agrupación se considera como ruido si se mira la clasificación o agrupación, pero en realidad podría no ser en realidad necesaria la exclusión de este valor, ya que no asegura nuestro patrón general en el aprendizaje. Sin embargo, eso es una gran discusión en el área de aprendizaje automático, es decir, ¿cuándo no se deben excluir determinados datos? Y ¿cuándo se guardan? Si se está buscando sobre aspectos de fraude o alteración de análisis, sin duda querremos mantenerlos. Así que hay un estándar llamado CRISP-DM, o Cross Industry Standard Process, para la minería de datos.

![](/img/4.png)

Este gráfico indica el proceso de minería de datos y los pasos que tendrá que pasar para su tarea de minería de datos. En primer lugar, y de suma importancia, es necesario entender por qué se hace esto, entender el negocio y las necesidades de este tipo de análisis, y después procederemos a observar los datos. La comprensión de los datos es una pieza muy importante: si tenemos todos los datos que necesitamos, o si necesitamos conocer y recopilar los datos en otro lugar. Una vez que entendemos lo que tenemos y lo que necesitamos, podemos pasar a la fase de preparación de datos. La fase de preparación de datos contiene: selección de datos, limpieza de datos, filtrado, selección de atributos, atributos de manipulación, selección de características, tal vez incluso aspectos de ingeniería. Una vez terminado, entonces pasamos a la construcción de modelos. Aquí es donde empezamos a hablar de clasificación y agrupación, asociación. Y vamos a pasar por esta fase muchas veces diferentes. Este es un proceso muy iterativo. Modelado, preparación de datos, comprensión de los datos. Va todo el camino de regreso a la comprensión del negocio. Se va a caminar y recorrer este proceso muchas veces. Una vez que creamos los modelos entonces pasamos a la fase de evaluación.

Vamos a hablar también un poco acerca de los diferentes tipos de funciones de evaluación. Una vez que los modelos se evalúan, lo que se precisa es volver atrás y recoger un subconjunto diferente de datos. Tal vez tener más datos o menos. Tal vez preparar los datos de manera diferente o utilizar un modelo diferente. Una vez que pasamos por este proceso varias veces, finalizamos porque nos gusta el conjunto o los modelos. A continuación, implementamos ese modelo y entrará en la producción. A medida que los nuevos datos vengan, puntuamos adicionalmente el modelo, de forma que nuevas predicciones se va a poder realizar.

Entonces podemos trabajar en la actualización de ese modelo y mantenerlo actualizado con nuevos datos, reentrenamiento, y pasar por ese proceso de nuevo.

#### La evaluación del modelo y su validación

